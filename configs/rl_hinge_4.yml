experiment:
  name:
  results_dir: sweep_runs/rl_hinge_4

wandb:
  init_kwargs:
    mode: online # options: online, offline, disabled
    project: ligdiff_cluster
    group: rl_hinge_4
  watch_model: False
  watch_kwargs:
    log: all # (str) One of "gradients", "parameters", "all", or None
    log_freq: 1000 # (int) log gradients and parameters every N batches

dataset:
  location: 'data/crossdocked_processed/'
  rec_elements: ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I', 'B'] # ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I', 'Mg', 'Mn', 'Zn', 'Ca', 'Fe', 'B']
  lig_elements: ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I', 'B']
  remove_hydrogen: True
  min_ligand_atoms: 8 # minimum number of atoms in a ligand, skip all smaller ligands in the dataset when processing
  pocket_edge_algorithm: 'bruteforce-blas'
  lig_box_padding: 8 # angstroms
  pocket_cutoff: 5 # angstroms
  receptor_k: 8
  dataset_size: # used only for debugging
  use_boltzmann_ot: False

rec_encoder:
  n_keypoints: 20
  n_convs: 8
  hidden_n_node_feat: 128 
  out_n_node_feat: 128
  use_tanh: False
  coords_range: 10
  kp_feat_scale: 1.0
  use_keypoint_feat_mha: False
  feat_mha_heads: 3

diffusion:
  n_timesteps: 1000
  precision: 1.0e-5
  lig_feat_norm_constant: 1
  rl_dist_threshold: 3

dynamics:
  n_layers: 6
  hidden_nf: 256
  receptor_keypoint_k: 6
  ligand_k: 8
  use_tanh: False # whether to use tanh activation for coordinate updates


rec_encoder_loss:
  loss_type: 'optimal_transport' # can be optimal_transport or gaussian_repulsion, or hinge

training:
  rec_encoder_loss_weight: 0.1
  rl_hinge_loss_weight: 1
  learning_rate: 1.0e-4
  weight_decay: 1.0e-12
  clip_grad: True
  clip_value: 1.5
  epochs: 300
  batch_size: 64
  test_interval: 1 # measured in epochs
  train_metrics_interval: 0.1 # epochs
  save_interval: 10 # epoch
  sample_interval: 10 # number of epochs between sampling/testing molecules
  test_epochs: 4 # number of epochs to run when evaluating on the test set
  num_workers: 0
  cyclic_lr: # settings regarding a cyclic learning rate
    use_cyclic_lr: False
    base_lr: 1.0e-5
    max_lr: 1.0e-4
    step_size_up_frac: 0.25 # we specify step_size_up as a fraction of iterations_per_epoch
  lambda_lr: # settings for using the lambdaLR schule
    use_lambda_lr: True
    epoch_start: 100
    last_epoch: 200
    multiplier: 0.98

sampling_config:
  n_receptors: 10
  n_replicates: 15
  rec_enc_batch_size: 64
  diff_batch_size: 64