experiment:
  name: baseline
  results_dir: experiments/

wandb:
  init_kwargs:
    mode: disabled # options: online, offline, disabled
    project: ligdiff
    group: 
  watch_model: False
  watch_kwargs:
    log: # (str) One of "gradients", "parameters", "all", or None
    log_freq: 100 # (int) log gradients and parameters every N batches

dataset:
  location: 'data/crossdock_mini/'
  rec_elements: ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I', 'B'] # ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I', 'Mg', 'Mn', 'Zn', 'Ca', 'Fe', 'B']
  lig_elements: ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I', 'B']
  remove_hydrogen: True
  min_ligand_atoms: 8 # minimum number of atoms in a ligand, skip all smaller ligands in the dataset when processing
  pocket_edge_algorithm: 'bruteforce-blas'
  lig_box_padding: 8 # angstroms
  pocket_cutoff: 5 # angstroms
  receptor_k: 8
  dataset_size: 50 # used only for debugging
  use_boltzmann_ot: False

rec_encoder:
  n_keypoints: 20
  n_convs: 6
  hidden_n_node_feat: 128 
  out_n_node_feat: 128
  use_tanh: True
  coords_range: 10
  kp_feat_scale: 1.0
  use_keypoint_feat_mha: True
  feat_mha_heads: 3

diffusion:
  n_timesteps: 1000
  keypoint_centered: False # when t=T, are atoms centered at COM of binding pocket keypoints? If true, then yes, if false, then atoms are at COM of binding pocket atoms when t=T
  precision: 1.0e-5
  lig_feat_norm_constant: 1

dynamics:
  n_layers: 6
  hidden_nf: 256
  receptor_keypoint_k: 6
  ligand_k: 8
  use_tanh: True # whether to use tanh activation for coordinate updates 


rec_encoder_loss:
  loss_type: 'gaussian_repulsion' # can be optimal_transport or gaussian_repulsion

training:
  rec_encoder_loss_weight: 0.1
  learning_rate: 1.0e-4
  weight_decay: 1.0e-12
  clip_grad: False
  clip_value: 1.5
  epochs: 3
  batch_size: 8
  test_interval: 10 # measured in epochs
  train_metrics_interval: 0.5 # epochs
  save_interval: 10 # epoch
  sample_interval: 10 # number of epochs between sampling/testing molecules
  test_epochs: 2 # number of epochs to run when evaluating on the test set
  num_workers: 1
  cyclic_lr: # settings regarding a cyclic learning rate
    use_cyclic_lr: False
    base_lr: 1.0e-5
    max_lr: 1.0e-4
    step_size_up_frac: 0.25 # we specify step_size_up as a fraction of iterations_per_epoch
  lambda_lr: # settings for using the lambdaLR schule
    use_lambda_lr: True
    epoch_start: 0
    last_epoch: 10
    multiplier: 0.98

sampling_config:
  n_receptors: 2
  n_replicates: 2
  rec_enc_batch_size: 32
  diff_batch_size: 32